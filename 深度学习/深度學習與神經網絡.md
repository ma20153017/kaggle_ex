以下是關於 **深度學習與神經網絡（PyTorch）** 的詳細介紹，包括原理、用法以及代碼示例。

---

## **1. 神經網絡**

神經網絡是模仿生物神經系統的結構，用於處理非線性問題。它由多層神經元組成，通過學習權重和偏置來逼近數據的目標函數。

---

### **1.1 梯度下降（Gradient Descent）**

#### **原理**
- **梯度下降** 是一種優化算法，用於最小化損失函數。
- 通過計算損失函數對參數的偏導數（梯度），沿梯度的負方向更新權重：
  **w = w - η * ∇L(w)**  
  其中 `η` 是學習率，`∇L(w)` 是損失函數對權重的梯度。

#### **代碼示例**
PyTorch 中的梯度下降通過 `torch.autograd` 自動求導完成：

```python
import torch

# 初始化權重
w = torch.tensor(2.0, requires_grad=True)

# 定義損失函數
loss_fn = lambda w: (w - 3) ** 2  # 簡單的二次損失

# 執行梯度下降
for i in range(10):
    loss = loss_fn(w)
    loss.backward()  # 計算梯度
    with torch.no_grad():  # 禁止梯度追蹤以更新權重
        w -= 0.1 * w.grad  # 梯度下降更新
        w.grad.zero_()  # 清空梯度
    print(f"Step {i+1}: w = {w.item()}, loss = {loss.item()}")
```

---

### **1.2 反向傳播（Backpropagation）**

#### **原理**
- **反向傳播** 是神經網絡訓練的核心算法，通過鏈式法則計算損失函數相對於權重的梯度。
- PyTorch 自動執行反向傳播，無需手動計算偏導數。

#### **核心步驟**
1. **前向傳播**：計算損失。
2. **反向傳播**：計算梯度。
3. **參數更新**：根據優化算法（如 SGD 或 Adam）更新權重。

#### **代碼示例**
以下示例展示如何使用反向傳播更新神經網絡的參數：

```python
import torch
import torch.nn as nn

# 定義簡單的線性模型
model = nn.Linear(1, 1)  # 單輸入單輸出
loss_fn = nn.MSELoss()  # 使用均方誤差損失函數
optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # 隨機梯度下降

# 訓練數據
X = torch.tensor([[1.0], [2.0], [3.0]])
y = torch.tensor([[2.0], [4.0], [6.0]])

# 訓練過程
for epoch in range(10):
    # 前向傳播
    y_pred = model(X)
    loss = loss_fn(y_pred, y)

    # 反向傳播
    optimizer.zero_grad()  # 清空梯度
    loss.backward()  # 計算梯度
    optimizer.step()  # 更新參數

    print(f"Epoch {epoch+1}: Loss = {loss.item()}")
```

---

### **1.3 激活函數（Activation Functions）**

#### **原理**
激活函數引入非線性，使神經網絡能夠擬合複雜的數據分佈。

#### **常見激活函數**
1. **Sigmoid**:  
   \( \text{Sigmoid}(x) = \frac{1}{1 + e^{-x}} \)  
   適用於輸出概率，但容易導致梯度消失。
2. **ReLU (Rectified Linear Unit)**:  
   \( \text{ReLU}(x) = \max(0, x) \)  
   常用於隱藏層，計算簡單且有效。
3. **Tanh**:  
   \( \text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \)  
   使輸出範圍在 \([-1, 1]\)。
4. **Softmax**:  
   將輸出轉換為概率分佈，常用於多分類問題。

#### **代碼示例**
激活函數可以在 PyTorch 中直接使用：

```python
import torch.nn.functional as F

# 範例數據
x = torch.tensor([-1.0, 0.0, 1.0])

# 激活函數的應用
print("Sigmoid:", torch.sigmoid(x))
print("ReLU:", F.relu(x))
print("Tanh:", torch.tanh(x))
```

---

### **1.4 損失函數（Loss Functions）**

#### **原理**
損失函數衡量模型預測值與真實值的差異，目的是最小化損失函數。

#### **常見損失函數**
1. **均方誤差 (MSE)**:  
   \( L = \frac{1}{n} \sum (y_{\text{true}} - y_{\text{pred}})^2 \)  
   適用於回歸問題。
2. **交叉熵損失 (Cross-Entropy Loss)**:  
   \( L = - \sum y_{\text{true}} \log(y_{\text{pred}}) \)  
   適用於分類問題。
   
#### **代碼示例**
```python
# 定義損失函數
mse_loss = nn.MSELoss()
cross_entropy_loss = nn.CrossEntropyLoss()

# 範例數據
y_true = torch.tensor([1.0, 2.0, 3.0])
y_pred = torch.tensor([1.5, 2.5, 3.5])

# 計算損失
print("MSE Loss:", mse_loss(y_pred, y_true))

# 交叉熵損失（適用於分類）
y_true = torch.tensor([0])  # 類別標籤
y_pred = torch.tensor([[2.0, 1.0]])  # 預測分數
print("Cross-Entropy Loss:", cross_entropy_loss(y_pred, y_true))
```

---

## **2. 深度學習**

深度學習是由多層神經網絡組成的學習系統，用於處理複雜的數據模式。

---

### **2.1 多層感知機（MLP）**

#### **原理**
多層感知機是全連接神經網絡的基本結構，由多層神經元組成，通過激活函數引入非線性。

#### **代碼示例**
```python
import torch.nn as nn

# 定義 MLP
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden = nn.Linear(2, 4)  # 隱藏層
        self.output = nn.Linear(4, 1)  # 輸出層

    def forward(self, x):
        x = torch.relu(self.hidden(x))  # 激活函數
        x = self.output(x)
        return x

# 創建模型
model = MLP()
print(model)
```

---

### **2.2 優化算法（SGD 和 Adam）**

#### **原理**
- **SGD (Stochastic Gradient Descent)**:  
  使用小批量數據計算梯度，更新參數。
- **Adam**:  
  自適應學習率算法，結合了動量和 RMSProp。

#### **代碼示例**
```python
# 使用 SGD 優化器
optimizer_sgd = torch.optim.SGD(model.parameters(), lr=0.01)

# 使用 Adam 優化器
optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.001)
```

---

### **2.3 學習率（Learning Rate）**

#### **原理**
- 學習率控制參數每次更新的步長。
- 學習率過大：可能無法收斂；過小：收斂速度慢。

#### **學習率調整**
1. **固定學習率**：在訓練過程中保持不變。
2. **動態調整學習率**：根據 epoch 或損失自適應調整。

#### **代碼示例**
```python
from torch.optim.lr_scheduler import StepLR

# 創建學習率調度器
scheduler = StepLR(optimizer_adam, step_size=10, gamma=0.1)

# 每個 epoch 調整學習率
for epoch in range(30):
    # 模型訓練代碼省略
    scheduler.step()  # 調整學習率
    print(f"Epoch {epoch+1}, Learning Rate: {scheduler.get_last_lr()}")
```

---

以上介紹涵蓋了 **神經網絡與深度學習** 的核心概念、原理解析和 PyTorch 實現。若需要更詳細的案例（如 CNN、RNN），請進一步告知！
