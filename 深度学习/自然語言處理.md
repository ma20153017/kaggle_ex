以下是**自然語言處理（NLP）**在 PyTorch 下的重點知識與代碼示例：

---

## 1. 詞嵌入 / 詞向量（word2vec）

### 原理解析
- **詞嵌入**：將離散的詞語映射到連續空間的低維向量，用於捕捉語義相似性。
- **word2vec**：通過上下文學習詞的分佈式表示，常見兩種結構：
  - **CBOW**（Continuous Bag of Words）：用上下文預測中心詞
  - **Skip-gram**：用中心詞預測上下文

### PyTorch 內建詞嵌入
PyTorch 使用 `nn.Embedding` 層進行詞嵌入。

#### 代碼示例
```python
import torch
import torch.nn as nn

vocab_size = 10000
embedding_dim = 300
embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)

input_ids = torch.LongTensor([1, 5, 8, 20])  # 假設一個詞id序列
embedded = embedding(input_ids)  # shape: (4, 300)
print(embedded.shape)
```

#### 使用預訓練詞向量（如 GloVe）
```python
import numpy as np

# 假設 glove_embeddings 是 (vocab_size, embedding_dim) 的 numpy array
embedding = nn.Embedding.from_pretrained(torch.FloatTensor(glove_embeddings))
```

---

## 2. Transformer 基礎（Attention 機制）

### 原理解析

- **Attention**：模型根據查詢（Query）對所有鍵（Key）計算權重，聚合所有值（Value）得到表示。
- **Self-Attention**：序列中每個詞對整個序列其他詞計算相關性，捕捉遠距依賴。
- **Transformer**：堆疊多層 self-attention 和前饋網絡，無需 RNN/CNN，可並行計算。

#### Attention 計算公式
$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

### 簡單 Self-Attention 代碼（PyTorch）
```python
class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        super().__init__()
        self.query = nn.Linear(embed_dim, embed_dim)
        self.key = nn.Linear(embed_dim, embed_dim)
        self.value = nn.Linear(embed_dim, embed_dim)
        self.softmax = nn.Softmax(dim=-1)
    
    def forward(self, x):
        Q = self.query(x)
        K = self.key(x)
        V = self.value(x)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / (Q.size(-1) ** 0.5)
        attn_weights = self.softmax(scores)
        out = torch.matmul(attn_weights, V)
        return out, attn_weights

# 假設 batch_size=2, seq_len=5, embed_dim=16
x = torch.randn(2, 5, 16)
sa = SelfAttention(embed_dim=16)
output, weights = sa(x)
```

---

## 3. 預訓練 NLP 模型（BERT、GPT）

### 原理解析

- **BERT**（Bidirectional Encoder Representations from Transformers）：
  - 採用 Transformer Encoder 結構。
  - 雙向上下文，主要用於理解類任務（分類、問答）。
  - 預訓練任務：Masked Language Model（MLM）、Next Sentence Prediction（NSP）。

- **GPT**（Generative Pretrained Transformer）：
  - 採用 Transformer Decoder 結構。
  - 只用左側上下文（自回歸），主要用於生成類任務（文本生成）。
  - 預訓練任務：自回歸語言模型。

### 使用 HuggingFace Transformers

#### 1. BERT 應用（文本分類）
```python
from transformers import BertTokenizer, BertForSequenceClassification

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello, this is a test!", return_tensors="pt")
outputs = model(**inputs)
logits = outputs.logits
```

#### 2. GPT 應用（文本生成）
```python
from transformers import GPT2Tokenizer, GPT2LMHeadModel

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

input_ids = tokenizer.encode("Once upon a time", return_tensors='pt')
outputs = model.generate(input_ids, max_length=30)
print(tokenizer.decode(outputs[0]))
```

---

## 總結表格

| 技術           | 核心原理                     | PyTorch API / 代碼關鍵    | 典型應用                   |
|----------------|------------------------------|---------------------------|----------------------------|
| 詞嵌入         | 分佈式詞向量，捕捉語義       | `nn.Embedding`            | 文本分類、序列建模         |
| Attention      | 動態聚合信息，捕捉依賴       | 自定義/`nn.MultiheadAttention` | Transformer/BERT/GPT      |
| BERT           | 雙向 Transformer Encoder     | `transformers`（HuggingFace） | 文本理解、分類、問答       |
| GPT            | 單向 Transformer Decoder     | `transformers`（HuggingFace） | 文本生成、對話             |

---

## 補充說明

- **自定義詞嵌入**可用於小型專有語料，更大數據集建議直接用 GloVe/word2vec。
- **Transformer**是現代 NLP 核心，理解 Attention 對進階應用（如 BERT 微調、生成式任務）至關重要。
- **HuggingFace Transformers** 極大簡化了 BERT/GPT 等模型的實踐，推薦用於各類 NLP 項目。

---

如需**更詳細的代碼或具體應用實例**（如文本分類、命名實體識別等），請告知需求！
